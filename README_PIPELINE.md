# ğŸš€ Pipeline MLOps - CI/CD avec GitHub Actions et MLflow

Pipeline complet d'entraÃ®nement, validation et dÃ©ploiement automatique de modÃ¨les ML.

## ğŸ“‹ Table des MatiÃ¨res

- [Architecture](#architecture)
- [Setup Initial](#setup-initial)
- [Comment Ã§a marche](#comment-Ã§a-marche)
- [DÃ©clencheurs du Pipeline](#dÃ©clencheurs-du-pipeline)
- [Tester en Local](#tester-en-local)
- [Configuration GitHub](#configuration-github)

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     PIPELINE CI/CD MLOPS                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

1. DÃ‰TECTION DE NOUVELLES DONNÃ‰ES
   â”œâ”€â”€ VÃ©rifier si nouvelles apps >= 100
   â””â”€â”€ DÃ©clencher pipeline si seuil atteint

2. RÃ‰ENTRAÃNEMENT
   â”œâ”€â”€ Charger donnÃ©es (anciennes + nouvelles)
   â”œâ”€â”€ EntraÃ®ner plusieurs modÃ¨les (RandomForest, LogReg)
   â”œâ”€â”€ SÃ©lectionner le meilleur
   â””â”€â”€ Logger dans MLflow

3. VALIDATION & COMPARAISON
   â”œâ”€â”€ Comparer avec modÃ¨le en production
   â”œâ”€â”€ Calculer amÃ©lioration
   â””â”€â”€ Score de confiance

4. DÃ‰CISION AUTOMATIQUE
   â”œâ”€â”€ Score >= 70%: âœ… DÃ©ploiement auto
   â”œâ”€â”€ Score 50-70%: ğŸŸ¡ Validation manuelle
   â””â”€â”€ Score < 50%: âŒ Rejet

5. DÃ‰PLOIEMENT PROGRESSIF
   â”œâ”€â”€ Staging â†’ Tests
   â”œâ”€â”€ Canary (5% trafic) â†’ Monitoring 5min
   â”œâ”€â”€ Rollout progressif (25% â†’ 50% â†’ 100%)
   â””â”€â”€ Production complÃ¨te

6. MONITORING
   â”œâ”€â”€ MÃ©triques en temps rÃ©el
   â”œâ”€â”€ Alertes si dÃ©gradation
   â””â”€â”€ Rollback automatique si nÃ©cessaire
```

## ğŸš¦ Setup Initial

### 1. Cloner et Configurer

```bash
# Cloner le repo
git clone <votre-repo>
cd mlops-jupyter

# CrÃ©er l'environnement virtuel
python -m venv .venv
source .venv/bin/activate  # ou .venv\Scripts\activate sur Windows

# Installer les dÃ©pendances
pip install -r requirements.txt
```

### 2. Lancer MLflow en Local

```bash
# DÃ©marrer le serveur MLflow
mlflow server \
  --backend-store-uri sqlite:///mlflow.db \
  --default-artifact-root ./mlflow/artifacts \
  --host 0.0.0.0 \
  --port 5000

# AccÃ©der Ã  l'interface: http://localhost:5000
```

### 3. PrÃ©parer les DonnÃ©es

```bash
# Copier vos donnÃ©es
cp your_data.csv data/googleplaystore_clean.csv
```

## ğŸ¯ Comment Ã§a marche

### DÃ©clencheurs Automatiques

Le pipeline se dÃ©clenche automatiquement dans 3 cas:

1. **Push sur main** (aprÃ¨s merge d'un PR)
   ```bash
   git add .
   git commit -m "Ajout de nouvelles applications"
   git push origin main
   ```

2. **ProgrammÃ©** (tous les jours Ã  2h UTC)
   - Automatique via GitHub Actions

3. **Manuel** (via GitHub UI)
   - Aller dans Actions â†’ ML Pipeline â†’ Run workflow

### Workflow Complet

```bash
# Exemple: Ajouter de nouvelles donnÃ©es

# 1. Ajouter des donnÃ©es
echo "new_app_data" >> data/googleplaystore_clean.csv

# 2. Commit et push
git add data/
git commit -m "feat: ajout de 150 nouvelles applications"
git push origin main

# 3. GitHub Actions se dÃ©clenche automatiquement:
#    âœ… DÃ©tecte 150 nouvelles apps (>= seuil de 100)
#    âœ… RÃ©entraÃ®ne les modÃ¨les
#    âœ… Compare avec production
#    âœ… DÃ©cide du dÃ©ploiement (score 85/100)
#    âœ… DÃ©ploie en staging
#    âœ… Tests automatiques
#    âœ… DÃ©ploie en production (canary â†’ full)
#    âœ… Monitoring actif
#    âœ… Notification envoyÃ©e

# 4. VÃ©rifier dans GitHub Actions
#    - Voir les logs en temps rÃ©el
#    - TÃ©lÃ©charger les rapports
#    - Consulter les mÃ©triques MLflow
```

## ğŸ§ª Tester en Local

### Test du Pipeline Complet

```bash
# 1. VÃ©rifier les nouvelles donnÃ©es
python src/check_new_data.py

# 2. EntraÃ®ner le modÃ¨le
export MLFLOW_TRACKING_URI=http://localhost:5000
python src/train_pipeline.py

# 3. DÃ©cision de dÃ©ploiement
python src/deployment_decision.py

# 4. DÃ©ployer (si approuvÃ©)
python src/deploy.py --environment staging
python src/test_deployment.py --environment staging
python src/deploy.py --environment production --canary 0.05
python src/monitor_canary.py --duration 300
python src/deploy.py --environment production --canary 1.0

# 5. Notification
python src/notify.py --version v20260103 --accuracy 0.92 --improvement 0.015
```

### Test de Rollback

```bash
# Simuler un problÃ¨me et rollback
python src/deploy.py --rollback
python src/notify.py --rollback --reason "Performance dÃ©gradÃ©e"
```

## âš™ï¸ Configuration GitHub

### 1. CrÃ©er le Repository

```bash
# Initialiser Git
git init
git add .
git commit -m "Initial commit: MLOps pipeline"

# CrÃ©er le repo sur GitHub (via UI)
# Puis:
git remote add origin https://github.com/votre-username/mlops-jupyter.git
git branch -M main
git push -u origin main
```

### 2. Configuration des Secrets (optionnel)

Dans GitHub â†’ Settings â†’ Secrets â†’ Actions, ajouter:

```yaml
MLFLOW_TRACKING_URI: your-mlflow-server-url
SLACK_WEBHOOK: your-slack-webhook
EMAIL_SMTP: your-smtp-config
```

### 3. Activer GitHub Actions

- Aller dans l'onglet "Actions"
- Le workflow `.github/workflows/ml-pipeline.yml` est automatiquement dÃ©tectÃ©
- Cliquer sur "I understand, enable them"

## ğŸ“Š Monitoring avec MLflow

### AccÃ©der Ã  MLflow UI

```bash
# Local
http://localhost:5000

# Voir les runs
# Comparer les modÃ¨les
# TÃ©lÃ©charger les artifacts
```

### API MLflow

```python
import mlflow

# Charger un modÃ¨le
model = mlflow.sklearn.load_model("models:/production-model/latest")

# Faire une prÃ©diction
predictions = model.predict(X_new)
```

## ğŸ”§ Structure du Projet

```
mlops-jupyter/
â”œâ”€â”€ .github/
â”‚   â””â”€â”€ workflows/
â”‚       â””â”€â”€ ml-pipeline.yml       # Workflow GitHub Actions
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ check_new_data.py         # DÃ©tection nouvelles donnÃ©es
â”‚   â”œâ”€â”€ train_pipeline.py         # Pipeline d'entraÃ®nement
â”‚   â”œâ”€â”€ deployment_decision.py    # DÃ©cision auto dÃ©ploiement
â”‚   â”œâ”€â”€ deploy.py                 # Script de dÃ©ploiement
â”‚   â”œâ”€â”€ test_deployment.py        # Tests smoke
â”‚   â”œâ”€â”€ monitor_canary.py         # Monitoring canary
â”‚   â”œâ”€â”€ generate_report.py        # GÃ©nÃ©ration rapports
â”‚   â””â”€â”€ notify.py                 # Notifications
â”œâ”€â”€ data/
â”‚   â””â”€â”€ googleplaystore_clean.csv # DonnÃ©es
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ production_model.pkl      # ModÃ¨le en prod
â”‚   â”œâ”€â”€ candidate_model.pkl       # ModÃ¨le candidat
â”‚   â””â”€â”€ last_training_date.txt    # Tracking
â”œâ”€â”€ mlflow/
â”‚   â””â”€â”€ artifacts/                # Artifacts MLflow
â”œâ”€â”€ logs/
â”‚   â””â”€â”€ deployment.log            # Logs dÃ©ploiement
â”œâ”€â”€ reports/
â”‚   â””â”€â”€ report_*.json             # Rapports performance
â”œâ”€â”€ requirements.txt
â””â”€â”€ README_PIPELINE.md
```

## ğŸ“ˆ CritÃ¨res de DÃ©ploiement

### Score Automatique (sur 100 points)

| CritÃ¨re | Points | Condition |
|---------|--------|-----------|
| AmÃ©lioration > 1% | 40 | DiffÃ©rence accuracy |
| AmÃ©lioration > 0% | 20 | LÃ©gÃ¨re amÃ©lioration |
| Accuracy > 90% | 30 | QualitÃ© absolue |
| Accuracy > 80% | 20 | Bonne qualitÃ© |
| StabilitÃ© | 30 | MÃ©triques cohÃ©rentes |

### DÃ©cisions

- **Score â‰¥ 70**: âœ… DÃ©ploiement automatique
- **Score 50-69**: ğŸŸ¡ Validation manuelle requise
- **Score < 50**: âŒ DÃ©ploiement refusÃ©

## ğŸš¨ Rollback Automatique

Le rollback se dÃ©clenche automatiquement si:

- DÃ©ploiement Ã©choue
- Tests smoke Ã©chouent
- Performance baisse > 2% aprÃ¨s dÃ©ploiement
- Erreur systÃ¨me dÃ©tectÃ©e

```bash
# Rollback manuel
python src/deploy.py --rollback
```

## ğŸ“§ Notifications

Configurez les notifications dans `src/notify.py`:

- **Slack**: Webhook
- **Email**: SMTP
- **Teams**: Webhook
- **PagerDuty**: API

## ğŸ“ Workflow RecommandÃ©

### Development

```bash
# 1. CrÃ©er une branche
git checkout -b feature/new-model

# 2. DÃ©velopper
# - Modifier le preprocessing
# - Tester de nouveaux modÃ¨les
# - AmÃ©liorer les features

# 3. Tester localement
python src/train_pipeline.py

# 4. Commit et push
git add .
git commit -m "feat: nouveau feature engineering"
git push origin feature/new-model

# 5. CrÃ©er une Pull Request
# - Review du code
# - Tests automatiques

# 6. Merge vers main
# - Pipeline CI/CD se dÃ©clenche automatiquement
```

## ğŸ’¡ Best Practices

1. **Versionner les modÃ¨les** avec MLflow
2. **Tester en staging** avant production
3. **Monitoring continu** post-dÃ©ploiement
4. **Garder des backups** (5 derniÃ¨res versions)
5. **Documenter** chaque dÃ©ploiement
6. **Rollback rapide** en cas de problÃ¨me

## ğŸ”— Ressources

- [MLflow Documentation](https://mlflow.org/docs/latest/index.html)
- [GitHub Actions](https://docs.github.com/en/actions)
- [Scikit-learn](https://scikit-learn.org/)

## ğŸ“ License

MIT

---

**CrÃ©Ã© avec â¤ï¸ pour le MLOps**
